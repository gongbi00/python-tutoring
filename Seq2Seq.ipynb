{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d4afebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch 1.2 버젼에는 Attention is All You Need 논문에 기반한 표준 트랜스포머(transformer) 모듈을 포함하고 있습니다.\n",
    "#트랜스포머 모델은 다양한 시퀀스-투-시퀀스 문제들에서 더 병렬화(parallelizable)가 가능하면서도 순환 신경망(RNN; Recurrent Neural Network)과\n",
    "#비교하여 더 나은 성능을 보임이 입증되었습니다. nn.Transformer 모듈은 입력(input) 과 출력(output) 사이의 전역적인 의존성\n",
    "#(global dependencies) 을 나타내기 위하여 (nn.MultiheadAttention 으로 구현된) 어텐션(attention) 메커니즘에 전적으로 의존합니다. \n",
    "#현재 nn.Transformer 모듈은 모듈화가 잘 되어 있어 단일 컴포넌트 (예. nn.TransformerEncoder ) 로 쉽게 적용 및 구성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91a2dd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "###모델 정의하기\n",
    "'''\n",
    "이 튜토리얼에서, 우리는 nn.TransformerEncoder 모델을 언어 모델링(language modeling) 과제에 대해서 학습시킬 것입니다. \n",
    "언어 모델링 과제는 주어진 단어 (또는 단어의 시퀀스) 가 다음에 이어지는 단어 시퀀스를 따를 가능성(likelihood)에 대한 확률을 \n",
    "할당하는 것입니다. 먼저, 토큰(token) 들의 시퀀스가 임베딩(embedding) 레이어로 전달되며, 이어서 포지셔널 인코딩(positional encoding) \n",
    "레이어가 각 단어의 순서를 설명합니다. (더 자세한 설명은 다음 단락을 참고해주세요.) nn.TransformerEncoder 는 여러 개의 \n",
    "nn.TransformerEncoderLayer 레이어로 구성되어 있습니다. nn.TransformerEncoder 내부의 셀프-어텐션(self-attention) 레이어들은 \n",
    "시퀀스 안에서의 이전 포지션에만 집중하도록 허용되기 때문에, 입력(input) 순서와 함께, 정사각 형태의 어텐션 마스크(attention mask)가 \n",
    "필요합니다. 언어 모델링 과제를 위해서, 미래의 포지션에 있는 모든 토큰들은 마스킹 되어야(가려져야) 합니다. 실제 단어를 얻기 위해서, \n",
    "nn.TransformerEncoder 의 출력은 로그-소프트맥스(log-Softmax) 로 이어지는 최종 선형(Linear) 레이어로 전달 됩니다.\n",
    "'''\n",
    "\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "394754cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PositionalEncoding 모듈은 시퀀스 안에서 토큰의 상대적인 또는 절대적인 포지션에 대한 어떤 정보를 주입합니다. 포지셔널 인코딩은 임베딩과 \n",
    "#합칠 수 있도록 똑같은 차원을 가집니다. 여기에서, 우리는 다른 주파수(frequency) 의 sine 과 cosine 함수를 사용합니다.\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63d7b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "###데이터 로드하고 배치 만들기\n",
    "#이 튜토리얼에서는 torchtext 를 사용하여 Wikitext-2 데이터셋을 생성합니다. 단어 오브젝트는 훈련 데이터셋(train dataset)에 \n",
    "#의하여 만들어지고, 토큰(token)을 텐서(tensor)로 수치화하는데 사용됩니다. Wikitext-2에서 보기 드믄 토큰(rare token)은 <unk> 로 표현됩니다.\n",
    "\n",
    "#주어진 1D 벡터의 시퀀스 데이터에서, batchify() 함수는 데이터를 batch_size 컬럼들로 정렬합니다. 만약 데이터가 batch_size 컬럼으로 나누어 \n",
    "#떨어지지 않으면, 데이터를 잘라내서 맞춥니다. 예를 들어 (총 길이 26의) 알파벳을 데이터로 보고 batch_size=4 일 때, \n",
    "#알파벳은 길이가 6인 4개의 시퀀스로 나눠집니다:\n",
    "\n",
    "#배치 작업(batching)은 더 많은 병렬 처리를 가능하게 하지만, 모델이 독립적으로 각 컬럼들을 취급해야 함을 뜻합니다; \n",
    "#예를 들어, 위 예제에서 G 와 F 의 의존성(dependance)은 학습되지 않습니다.\n",
    "\n",
    "\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d7e0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##입력(input) 과 타겟(target) 시퀀스를 생성하기 위한 함수들\n",
    "#get_batch() 함수는 트랜스포머 모델을 위한 입력-타겟 시퀀스 쌍(pair)을 생성합니다. 이 함수는 소스 데이터를 bptt 길이를 가진 덩어리로 세분화 \n",
    "#합니다. 언어 모델링 과제를 위해서, 모델은 다음 단어인 Target 이 필요 합니다. 예를 들어, bptt 의 값이 2 라면, 우리는 i = 0 일 때 \n",
    "#다음의 2 개의 변수(Variable) 를 얻을 수 있습니다:\n",
    "#변수 덩어리는 트랜스포머 모델의 S 차원과 일치하는 0 차원에 해당합니다. 배치 차원 N 은 1 차원에 해당합니다.\n",
    "\n",
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4ed7efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###인스턴스(instance) 초기화하기\n",
    "#모델의 하이퍼파라미터(hyperparameter)는 아래와 같이 정의됩니다. 단어 사이즈는 단어 오브젝트의 길이와 일치 합니다.\n",
    "ntokens = len(vocab) # 단어 사전(어휘집)의 크기\n",
    "emsize = 200 # 임베딩 차원\n",
    "d_hid = 200 # nn.TransformerEncoder 에서 피드포워드 네트워크(feedforward network) 모델의 차원\n",
    "nlayers = 2 # nn.TransformerEncoder 내부의 nn.TransformerEncoderLayer 개수\n",
    "nhead = 2 # nn.MultiheadAttention의 헤드 개수\n",
    "dropout = 0.2 # 드랍아웃(dropout) 확률\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "246b9a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###모델 실행하기\n",
    "#CrossEntropyLoss 를 SGD (확률적 경사 하강법) 옵티마이저(optimizer)와 함께 사용하였습니다. 학습률(learning rate)는 5.0으로 초기화하였으며 \n",
    "#StepLR 스케쥴을 따릅니다. 학습하는 동안, nn.utils.clip_grad_norm_ 을 사용하여 기울기(gradient)가 폭발(exploding)하지 않도록 합니다.\n",
    "\n",
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # 학습률(learning rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # 학습 모드 시작\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # 마지막 배치에만 적용\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # 평가 모드 시작\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b773e01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 39.99 | loss  8.27 | ppl  3912.78\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 11.90 | loss  6.92 | ppl  1007.99\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 11.94 | loss  6.46 | ppl   641.61\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 11.90 | loss  6.32 | ppl   554.67\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 11.90 | loss  6.20 | ppl   491.82\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 11.91 | loss  6.16 | ppl   475.05\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 11.92 | loss  6.12 | ppl   454.24\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 11.90 | loss  6.12 | ppl   454.96\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 11.90 | loss  6.03 | ppl   417.75\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 11.99 | loss  6.03 | ppl   414.25\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 11.88 | loss  5.90 | ppl   366.15\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 11.90 | loss  5.98 | ppl   393.77\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 12.12 | loss  5.96 | ppl   388.62\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 12.02 | loss  5.89 | ppl   362.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 42.32s | valid loss  5.83 | valid ppl   338.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 11.97 | loss  5.88 | ppl   356.74\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 11.98 | loss  5.86 | ppl   349.00\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 11.97 | loss  5.68 | ppl   291.66\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 11.95 | loss  5.71 | ppl   303.21\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 11.92 | loss  5.66 | ppl   288.32\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 11.88 | loss  5.69 | ppl   294.52\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 11.95 | loss  5.69 | ppl   295.34\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 11.91 | loss  5.71 | ppl   302.87\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 11.86 | loss  5.66 | ppl   286.50\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 11.93 | loss  5.68 | ppl   292.89\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 11.90 | loss  5.55 | ppl   258.13\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 11.98 | loss  5.64 | ppl   282.08\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 11.95 | loss  5.64 | ppl   281.38\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 12.16 | loss  5.57 | ppl   263.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 36.75s | valid loss  5.63 | valid ppl   279.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 12.03 | loss  5.60 | ppl   271.20\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 11.91 | loss  5.63 | ppl   278.32\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 11.90 | loss  5.43 | ppl   227.50\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 11.92 | loss  5.48 | ppl   240.26\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 11.89 | loss  5.44 | ppl   229.69\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 11.92 | loss  5.49 | ppl   241.05\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 11.93 | loss  5.50 | ppl   244.05\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 11.90 | loss  5.52 | ppl   250.15\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 11.93 | loss  5.47 | ppl   238.13\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 11.88 | loss  5.49 | ppl   241.23\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 11.90 | loss  5.36 | ppl   213.69\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 11.90 | loss  5.48 | ppl   239.73\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 11.87 | loss  5.48 | ppl   239.93\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 11.89 | loss  5.41 | ppl   222.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 36.66s | valid loss  5.58 | valid ppl   266.30\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#에포크 내에서 반복됩니다. 만약 검증 오차(validation loss) 가 우리가 지금까지 관찰한 것 중 최적이라면 모델을 저장합니다. \n",
    "#매 에포크 이후에 학습률을 조절합니다.\n",
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2b0bf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.49 | test ppl   242.86\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "#평가 데이터셋(test dataset)으로 모델을 평가하기\n",
    "test_loss = evaluate(best_model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b717b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
